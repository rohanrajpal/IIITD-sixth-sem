{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q3_Rohan_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNragGzDDVMA",
        "colab_type": "code",
        "outputId": "d054cf32-ad12-49ef-df55-fdaf20b35a13",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#@title Colab Mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_PATH = \"/content/drive/My Drive/Deep_Learning_Assignments/Assignment1/Q3/dlassignment1/q3\"\n",
        "# !git clone \"https://85869b109f25ac5241470005fcd8ead1673b1329@github.com/rohanrajpal/dlassignment1.git\"\n",
        "%cd \"{PROJECT_PATH}\"\n",
        "!git config --global user.email \"rohan17089@iiitd.ac.in\"\n",
        "!git config --global user.name \"Rohan Rajpal\"\n",
        "\n",
        "%load_ext autoreload\n",
        "# %cd .."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/Deep_Learning_Assignments/Assignment1/Q3/dlassignment1/q3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXBNLJR1eNhb",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "373c448a-8cf9-456c-e85a-f1a2418310a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#@title Password for SSHD\n",
        "#1 - setup ssh/user \n",
        "\n",
        "\n",
        "#Generate a random root password\n",
        "import random, string\n",
        "password = ''.join(random.choice(string.ascii_letters + string.digits) for i in range(30))\n",
        "\n",
        "\n",
        "#Setup sshd\n",
        "! apt-get install -qq -o=Dpkg::Use-Pty=0 openssh-server pwgen > /dev/null\n",
        "\n",
        "#Set root password\n",
        "! echo root:$password | chpasswd\n",
        "! mkdir -p /var/run/sshd\n",
        "! echo \"PermitRootLogin yes\" >> /etc/ssh/sshd_config\n",
        "! echo \"PasswordAuthentication yes\" >> /etc/ssh/sshd_config\n",
        "\n",
        "print(\"username: root\")\n",
        "print(\"password: \", password)\n",
        "\n",
        "#Run sshd\n",
        "get_ipython().system_raw('/usr/sbin/sshd -D &')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "username: root\n",
            "password:  JrBRhxSlTumD3wRyAb624xVB63L5Qv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRBXepW9cJXn",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "f7bff883-7e6c-4ede-fb83-7f545aa49ccf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#@title Setup Ngork\n",
        "# Collapse\n",
        "# 2 - Download Ngrok\n",
        "\n",
        "! wget -q -c -nc https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "! unzip -qq -n ngrok-stable-linux-amd64.zip\n",
        "\n",
        "# 3 - setup Ngrok - authtoken\n",
        "\n",
        "#Ask token\n",
        "print(\"Get your authtoken from https://dashboard.ngrok.com/auth\")\n",
        "import getpass\n",
        "authtoken = getpass.getpass()\n",
        "\n",
        "#Create tunnel\n",
        "get_ipython().system_raw('./ngrok authtoken $authtoken && ./ngrok tcp 22 &')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Get your authtoken from https://dashboard.ngrok.com/auth\n",
            "··········\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5vauzXPc0_B",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "50375650-3d90-4d70-c7f3-3c6e417ef99b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        }
      },
      "source": [
        "#@title Git Stuff\n",
        "# %cd q3\n",
        "# !git pull origin master\n",
        "# !git commit -m \"ok\"\n",
        "\n",
        "# !git status\n",
        "# !git add Q3.ipynb\n",
        "# !git add Q3-Rohan.ipynb\n",
        "# !git commit -m \"ok\"\n",
        "# !git add q3/Q3.ipynb\n",
        "# !git add q3/Old-Q3.ipynb\n",
        "# !git commit -m \"Add notebooks\"\n",
        "!git push origin master\n",
        "# !git stash \n",
        "# %cd \"/content/drive/My Drive/Deep_Learning_Assignments/Assignment1/Q3/dlassignment1/q3\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects:  14% (1/7)\u001b[K\rremote: Counting objects:  28% (2/7)\u001b[K\rremote: Counting objects:  42% (3/7)\u001b[K\rremote: Counting objects:  57% (4/7)\u001b[K\rremote: Counting objects:  71% (5/7)\u001b[K\rremote: Counting objects:  85% (6/7)\u001b[K\rremote: Counting objects: 100% (7/7)\u001b[K\rremote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1)\u001b[K\rremote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects:  25% (1/4)   \rUnpacking objects:  50% (2/4)   \rUnpacking objects:  75% (3/4)   \rUnpacking objects: 100% (4/4)   \rUnpacking objects: 100% (4/4), done.\n",
            "From https://github.com/rohanrajpal/dlassignment1\n",
            " * branch            master     -> FETCH_HEAD\n",
            "   00ac8de..7c352e4  master     -> origin/master\n",
            "Updating 00ac8de..7c352e4\n",
            "Fast-forward\n",
            " q3/visualize.py | 5 \u001b[32m+++\u001b[m\u001b[31m--\u001b[m\n",
            " 1 file changed, 3 insertions(+), 2 deletions(-)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWlXtn0RfCq7",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Imports\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from retinanet import model\n",
        "from retinanet.dataloader import CocoDataset,Normalizer,Resizer,Augmenter,AspectRatioBasedSampler,collater,CSVDataset, UnNormalizer\n",
        "from torchvision import transforms\n",
        "from retinanet import coco_eval,csv_eval\n",
        "from importlib import reload\n",
        "from torch.utils.data import DataLoader\n",
        "import json,csv\n",
        "from pprint import pprint\n",
        "import time\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import torch.optim as optim\n",
        "import argparse\n",
        "import collections\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHyuFKNTd3jl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH_TO_DATASET = \"/content/drive/My Drive/Deep_Learning_Assignments/Assignment1/Q3/assignment-data\"\n",
        "PATH_TO_WEIGHTS = PATH_TO_DATASET + \"/pretrained_weights_cleaned.pt\"\n",
        "ANNOTATION_CSV =\"/content/drive/My Drive/Deep_Learning_Assignments/Assignment1/Q3/assignment-data/annotations/instances_train_csv.csv\"\n",
        "CLASS_LIST =\"/content/drive/My Drive/Deep_Learning_Assignments/Assignment1/Q3/assignment-data/annotations/class_list.csv\"\n",
        "\n",
        "IMAGE_PATH = \"/content/drive/My Drive/Deep_Learning_Assignments/Assignment1/Q3/assignment-data/images/train\"\n",
        "INSTANCES_TRAIN_PATH = \"/content/drive/My Drive/Deep_Learning_Assignments/Assignment1/Q3/assignment-data/annotations/instances_train.json\"\n",
        "INSTANCES_TRAIN_ALT_PATH = \"/content/drive/My Drive/Deep_Learning_Assignments/Assignment1/Q3/assignment-data/annotations/instances_train_alt.json\"\n",
        "\n",
        "TRAIN_CSV = \"/content/drive/My Drive/Deep_Learning_Assignments/Assignment1/Q3/assignment-data/annotations/train.csv\"\n",
        "VAL_CSV = \"/content/drive/My Drive/Deep_Learning_Assignments/Assignment1/Q3/assignment-data/annotations/val.csv\"\n",
        "PATH_TO_FINETUNED_WEIGHTS = \"/content/drive/My Drive/Deep_Learning_Assignments/Assignment1/Q3/dlassignment1/q3/freeze_model/model_final.pt\"\n",
        "BEST_WEIGHTS = \"/content/drive/My Drive/Deep_Learning_Assignments/Assignment1/Q3/dlassignment1/q3/freeze_model/third/csv_retinanet_best_model_mAP.pt\"\n",
        "TO_SAVE_WT = \"/content/drive/My Drive/Deep_Learning_Assignments/Assignment1/Q3/dlassignment1/q3/freeze_model/third/best_weigths.pt\"\n",
        "\n",
        "# !cat \"{ANNOTATION_CSV}\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nV5QlR9LGwDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper functions\n",
        "def print_dict(model):\n",
        "  print(\"Model's state_dict:\")\n",
        "  for param_tensor in model.state_dict():\n",
        "      print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XteYyFH19YIk",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Clean given weights\n",
        "pretrained_dict = torch.load(PATH_TO_WEIGHTS)\n",
        "retina_dict = retinanet.state_dict()\n",
        "\n",
        "def print_last(comment):\n",
        "  print(comment,pretrained_dict[\"classificationModel.output.weight\"].size())\n",
        "  print(comment,pretrained_dict[\"classificationModel.output.bias\"].size())\n",
        "\n",
        "for k, v in pretrained_dict.items():\n",
        "  if k in retina_dict:\n",
        "    if k == \"classificationModel.output.weight\":\n",
        "      #do something\n",
        "      v = v[:72, :, :, :]\n",
        "    elif k ==\"classificationModel.output.bias\":\n",
        "      v = v[:72]\n",
        "\n",
        "    pretrained_dict[k] = v\n",
        "# 2. overwrite entries in the existing state dict\n",
        "retina_dict.update(pretrained_dict) \n",
        "# 3. load the new state dict\n",
        "retinanet.load_state_dict(pretrained_dict)\n",
        "\n",
        "# torch.save(retinanet.state_dict(), PATH_TO_DATASET + \"/pretrained_weights_cleaned.pt\")\n",
        "# print_dict(retinanet)\n",
        "# print(givenmodel['classificationModel.output.weight'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4eb9lyzi7FU",
        "colab_type": "code",
        "outputId": "9d94280a-59cf-45d7-fb53-5807efbd8291",
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "source": [
        "#@title Part 1: mAP and Confusion Matrix\n",
        "#@markdown The\n",
        "%autoreload 2\n",
        "\n",
        "dataset_val = CSVDataset(train_file = VAL_CSV, class_list=CLASS_LIST,\n",
        "                                     transform=transforms.Compose([Normalizer(),Resizer()]))\n",
        "retinanet = model.resnet50(num_classes=dataset_val.num_classes(),pretrained=False)\n",
        "\n",
        "use_gpu = True\n",
        "\n",
        "if use_gpu:\n",
        "    retinanet = retinanet.cuda()\n",
        "\n",
        "retinanet.load_state_dict(torch.load(PATH_TO_WEIGHTS))\n",
        "\n",
        "retinanet = torch.nn.DataParallel(retinanet).cuda()\n",
        "\n",
        "retinanet.training = False\n",
        "retinanet.eval()\n",
        "retinanet.module.freeze_bn()\n",
        "\n",
        "mAP = csv_eval.evaluate(dataset_val, retinanet)\n",
        "print(mAP)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "506/506\n",
            "mAP:\n",
            "bird: 0.0\n",
            "bobcat: 0.011557311096481603\n",
            "car: 0.4445338715265824\n",
            "cat: 0.007590241576459477\n",
            "raccoon: 0.05251340699101893\n",
            "rabbit: 0.03546910755148741\n",
            "coyote: 0.005090281855549031\n",
            "squirrel: 0.0\n",
            "{0: (0.0, 42.0), 1: (0.011557311096481603, 69.0), 2: (0.4445338715265824, 48.0), 3: (0.007590241576459477, 87.0), 4: (0.05251340699101893, 134.0), 5: (0.03546910755148741, 46.0), 6: (0.005090281855549031, 40.0), 7: (0.0, 56.0)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UA6SknCwxnuH",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Layer Freezing\n",
        "# retinanet = model.resnet50(num_classes=dataset_val.num_classes(),pretrained=False)\n",
        "# retinanet.load_state_dict(torch.load(PATH_TO_WEIGHTS))\n",
        "\n",
        "# for layer in retinanet.modules():\n",
        "#   if not isinstance(layer, model.ClassificationModel):\n",
        "#       for param in layer.parameters():\n",
        "#           param.requires_grad = False\n",
        "#       print(layer)\n",
        "#       print(\"-----------------\")\n",
        "\n",
        "# for layer in retinanet.modules():\n",
        "#   if not isinstance(layer, model.ClassificationModel):\n",
        "#       for param in layer.parameters():\n",
        "#           print(param.requires_grad)\n",
        "#       print(layer)\n",
        "#       print(\"-----------------\")\n",
        "# for param in retinanet.parameters():\n",
        "#   param.requires_grad = False\n",
        "# for param in retinanet.classificationModel.parameters():\n",
        "#   param.requires_grad = True\n",
        "# cnt=0\n",
        "# for name in retinanet.children():\n",
        "#     # print(name)\n",
        "#     cnt+=1\n",
        "# print(cnt)\n",
        "\n",
        "# num_ftrs = retinanet.fc.in_features\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFI_4H4jZdq6",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Make Reduced JSON\n",
        "\n",
        "# Make Reduced JSON\n",
        "\n",
        "IMAGE_PATH = \"/content/drive/My Drive/Deep_Learning_Assignments/Assignment1/Q3/assignment-data/images/train/\"\n",
        "\n",
        "# with open(INSTANCES_TRAIN_PATH) as f:\n",
        "#   data = json.load(f)\n",
        "\n",
        "# data_final = {}\n",
        "\n",
        "# data_final[\"info\"] = data[\"info\"]\n",
        "# data_final[\"categories\"] = data[\"categories\"]\n",
        "# data_final[\"images\"] = []\n",
        "# data_final[\"annotations\"] = []\n",
        "# import os\n",
        "\n",
        "# root = IMAGE_PATH\n",
        "# file_map = {}\n",
        "# for path, subdirs, files in os.walk(root):\n",
        "#     if files:\n",
        "#         print(os.path.join(path, min(files)))\n",
        "#         for file in files:\n",
        "#           # print(file)\n",
        "#           file_map[file] = os.path.join(path, file)\n",
        "\n",
        "# def inFolder(elem):\n",
        "#   if elem in file_map:\n",
        "#     return True\n",
        "#   # print(elem)\n",
        "#   return False\n",
        "\n",
        "# # remove images which arent there\n",
        "# for elem in data[\"images\"]:\n",
        "#   if inFolder(elem[\"file_name\"]):\n",
        "#     data_final[\"images\"].append(elem)\n",
        "\n",
        "# for elem in data[\"annotations\"]:\n",
        "#   if inFolder(elem[\"image_id\"]+\".jpg\"):\n",
        "#     data_final[\"annotations\"].append(elem)\n",
        "\n",
        "# print(len(data[\"images\"]),len(data[\"annotations\"]))\n",
        "# print(len(data_final[\"images\"]),len(data_final[\"annotations\"]))\n",
        "\n",
        "# f = open(INSTANCES_TRAIN_ALT_PATH,'w')\n",
        "# json.dump(data_final,f)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGmb-cE4pkPj",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "703d22b4-8229-4767-de2f-037ef7c1a810",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#@title Shift all files to train/\n",
        "\n",
        "# !ls -l \"/content/drive/My Drive/Deep_Learning_Assignments/Assignment1/Q3/assignment-data/annotations/\"\n",
        "# with open(INSTANCES_TRAIN_ALT_PATH) as f:\n",
        "#   data = json.load(f)\n",
        "# print(len(data[\"images\"]),len(data[\"annotations\"]))\n",
        "# !mv \"/content/drive/My Drive/Deep_Learning_Assignments/Assignment1/Q3/assignment-data/annotations/instances_train.json\" \"/content/drive/My Drive/Deep_Learning_Assignments/Assignment1/Q3/assignment-data/annotations/temp.json\"\n",
        "# !mv \"/content/drive/My Drive/Deep_Learning_Assignments/Assignment1/Q3/assignment-data/annotations/instances_train_alt.json\" \"/content/drive/My Drive/Deep_Learning_Assignments/Assignment1/Q3/assignment-data/annotations/instances_train.json\"\n",
        "\n",
        "# %cd \"/content/drive/My Drive/Deep_Learning_Assignments/Assignment1/Q3/assignment-data/images\"\n",
        "# !unzip -q train.zip\n",
        "\n",
        "\n",
        "# import shutil\n",
        "# for file in file_map.keys():\n",
        "#   oldpath = file_map[file]\n",
        "#   newpath = IMAGE_PATH\n",
        "#   print(oldpath,newpath)\n",
        "#   # command = \"mv \" + oldpath+\" \"+newpath\n",
        "#   # os.system(command)\n",
        "#   shutil.move(oldpath,newpath)\n",
        "  # !mv {oldpath} {newpath}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Deep_Learning_Assignments/Assignment1/Q3/assignment-data/images\n",
            "replace train/squirrel/5968bf89-23d2-11e8-a6a3-ec086b02610b.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLHK0AhMryzn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !ls -l \"/content/drive/My Drive/Deep_Learning_Assignments/Assignment1/Q3/assignment-data/images/train\"\n",
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQW1WBcWI7oF",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Generate annotation CSV\n",
        "\n",
        "class_map = {\n",
        "11:\"bird\",\n",
        "6:\"bobcat\",\n",
        "33:\"car\",\n",
        "16:\"cat\",\n",
        "3:\"raccoon\",\n",
        "10:\"rabbit\",\n",
        "9:\"coyote\",\n",
        "5:\"squirrel\"\n",
        "}\n",
        "\n",
        "with open(INSTANCES_TRAIN_PATH) as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "!rm -rf \"{ANNOTATION_CSV}\"\n",
        "f_csv = open(ANNOTATION_CSV, 'w', newline='')\n",
        "writer = csv.writer(f_csv)\n",
        "\n",
        "for anno in data[\"annotations\"]:\n",
        "  bbox = [int(x) for x in anno[\"bbox\"]]\n",
        "  bbox[2] = bbox[0] + bbox[2]\n",
        "  bbox[3] = bbox[1] + bbox[3]\n",
        "  if(anno[\"category_id\"] in class_map):\n",
        "    class_name = class_map[anno[\"category_id\"]]\n",
        "    img_path = IMAGE_PATH +\"/\"+class_name+\"/\" +anno[\"image_id\"] + \".jpg\"\n",
        "    # print(img_path)\n",
        "    writer.writerow([img_path,bbox[0],bbox[1],bbox[2],bbox[3],class_name])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u03Noj1GOqU3",
        "colab_type": "code",
        "outputId": "dca9da88-d99d-497e-b133-031848558853",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!wc -l \"/content/drive/My Drive/Deep_Learning_Assignments/Assignment1/Q3/assignment-data/annotations/instances_train_csv.csv\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1739 /content/drive/My Drive/Deep_Learning_Assignments/Assignment1/Q3/assignment-data/annotations/instances_train_csv.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn-NSgwdDdVh",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Train-Val Split\n",
        "# # Train Val split\n",
        "# from numpy.random import RandomState\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/My Drive/Deep_Learning_Assignments/Assignment1/Q3/assignment-data/annotations/instances_train_csv.csv',header=None)\n",
        "rng = RandomState()\n",
        "\n",
        "train = df.sample(frac=0.7, random_state=rng)\n",
        "test = df.loc[~df.index.isin(train.index)]\n",
        "\n",
        "\n",
        "# !rm -rf \"{TRAIN_CSV}\"\n",
        "# !rm -rf \"{VAL_CSV}\"\n",
        "train.to_csv(TRAIN_CSV,index=False,header=None)\n",
        "test.to_csv(VAL_CSV,index=False,header=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHZ2OsR3Sh1h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"----Whole----\")\n",
        "df = pd.read_csv('/content/drive/My Drive/Deep_Learning_Assignments/Assignment1/Q3/assignment-data/annotations/instances_train_csv.csv',header=None)\n",
        "print(df[5].value_counts())\n",
        "print(\"----Train-----\")\n",
        "df = pd.read_csv(TRAIN_CSV,header=None)\n",
        "print(df[5].value_counts())\n",
        "print(\"----Val----\")\n",
        "df = pd.read_csv(VAL_CSV,header=None)\n",
        "print(df[5].value_counts())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx4YIGNLFPVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cat \"{VAL_CSV}\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApBFbre3PBMc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f_csv = open(CLASS_LIST, 'w', newline='')\n",
        "writer = csv.writer(f_csv)\n",
        "lines = [\n",
        "[\"bird\",\"0\"],\n",
        "[\"bobcat\",\"1\"],\n",
        "[\"car\",\"2\"],\n",
        "[\"cat\",\"3\"],\n",
        "[\"raccoon\",\"4\"],\n",
        "[\"rabbit\",\"5\"],\n",
        "[\"coyote\",\"6\"],\n",
        "[\"squirrel\",\"7\"]\n",
        "]\n",
        "# for line in lines:\n",
        "#   writer.writerow(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3VNNRmkTfoW",
        "colab_type": "code",
        "outputId": "7d6f455c-166b-45d1-8ffe-ec557c64f2fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        }
      },
      "source": [
        "# !python visualize.py --dataset csv --csv_classes \"{CLASS_LIST}\" --csv_val \"{ANNOTATION_CSV}\" --model \"{PATH_TO_WEIGHTS}\"\n",
        "!python train.py --dataset csv --csv_train \"{ANNOTATION_CSV}\"  --csv_classes \"{CLASS_LIST}\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "No validation annotations provided.\n",
            "Num training images: 1693\n",
            "Epoch: 0 | Iteration: 0 | Classification loss: 1.14594 | Regression loss: 1.15697 | Running loss: 2.30291\n",
            "Epoch: 0 | Iteration: 1 | Classification loss: 1.15146 | Regression loss: 0.97798 | Running loss: 2.21617\n",
            "Epoch: 0 | Iteration: 2 | Classification loss: 1.15887 | Regression loss: 0.99550 | Running loss: 2.19557\n",
            "Epoch: 0 | Iteration: 3 | Classification loss: 1.13337 | Regression loss: 1.01302 | Running loss: 2.18327\n",
            "Epoch: 0 | Iteration: 4 | Classification loss: 1.14780 | Regression loss: 1.03639 | Running loss: 2.18346\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 173, in <module>\n",
            "    main()\n",
            "  File \"train.py\", line 131, in main\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/tensor.py\", line 195, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\", line 99, in backward\n",
            "    allow_unreachable=True)  # allow_unreachable flag\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMQUerLAWYk7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# !python visualize.py --dataset csv --csv_classes \"{CLASS_LIST}\"  --csv_val \"{VAL_CSV}\" --model \"{PATH_TO_VWEIGHTS}\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mEwlTHghhMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper for visualize\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81yKWKBD7Axx",
        "colab_type": "code",
        "outputId": "ea420bde-590a-4686-aaa7-603632ebd42d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        }
      },
      "source": [
        "# print(confusion_matrix([[0,0], [1,0], [1,0], [1,1]], [[0,0], [1,0], [0,1], [1,1]]))\n",
        "# !rm -rf images1/*\n",
        "# !ls images1/\n",
        "# !zip -r /content/file.zip ./images1/\n",
        "# from google.colab import files\n",
        "# files.download(TO_SAVE_WT)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-263e521f1731>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTO_SAVE_WT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0;34m'port'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m       \u001b[0;34m'path'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m       \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m   })\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8VtES-pf_ee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def draw_caption(image, box, caption):\n",
        "  b = np.array(box).astype(int)\n",
        "  cv2.putText(image, caption, (b[0], b[1] - 10), cv2.FONT_HERSHEY_PLAIN, 1, (0, 0, 0), 2)\n",
        "  cv2.putText(image, caption, (b[0], b[1] - 10), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 1)\n",
        "\n",
        "parser = {\"csv_classes\":CLASS_LIST, \"csv_val\":VAL_CSV, \"model_finetuned\":BEST_WEIGHTS, \"model_raw\": PATH_TO_WEIGHTS}\n",
        "dataset_val = CSVDataset(train_file=parser[\"csv_val\"], class_list=parser[\"csv_classes\"], transform=transforms.Compose([Normalizer(), Resizer()]))\n",
        "\n",
        "sampler_val = AspectRatioBasedSampler(dataset_val, batch_size=1, drop_last=False)\n",
        "dataloader_val = DataLoader(dataset_val, num_workers=1, collate_fn=collater, batch_sampler=sampler_val)\n",
        "\n",
        "retinanet_raw = model.resnet50(num_classes=8)\n",
        "\n",
        "retinanet_raw.load_state_dict(torch.load(parser[\"model_raw\"]))\n",
        "retinanet_finetuned = torch.load(parser[\"model_finetuned\"])\n",
        "torch.save(retinanet_finetuned.state_dict(), TO_SAVE_WT)\n",
        "\n",
        "\n",
        "def get_bbox(retinanet,img,bcolor,data,caption):\n",
        "  scores, classification, transformed_anchors = retinanet(data['img'].cuda().float())\n",
        "  # print(\"--------Classification-----------\")\n",
        "  # print(classification.shape,classification)\n",
        "  # print(\"--------Scores----------\")\n",
        "  # print(scores.shape,scores)\n",
        "  # print(\"---------Transformed_Idxs-------\")\n",
        "  # print(transformed_anchors.shape, transformed_anchors)\n",
        "  idxs = np.where(scores.cpu()>0.5)\n",
        "  # print(idxs[0])\n",
        "  # bbox = transformed_anchors[idxs[0][j], :]\n",
        "  class_labels = []\n",
        "  for j in range(idxs[0].shape[0]):\n",
        "      bbox = transformed_anchors[idxs[0][j], :]\n",
        "      x1 = int(bbox[0])\n",
        "      y1 = int(bbox[1])\n",
        "      x2 = int(bbox[2])\n",
        "      y2 = int(bbox[3])\n",
        "      label_name = dataset_val.labels[int(classification[idxs[0][j]])]\n",
        "      draw_caption(img, (x1, y1, x2, y2), label_name)\n",
        "      class_labels.append(label_name)\n",
        "      cv2.rectangle(img, (x1, y1), (x2, y2), color=bcolor, thickness=2)\n",
        "      # print(caption,label_name)\n",
        "  return class_labels\n",
        "\n",
        "\n",
        "def equalize(pred, truth):\n",
        "  diff = len(pred) - len(truth)\n",
        "  nothing_pad = [\"nothing\" for x in range(abs(diff))]\n",
        "  if(diff > 0):\n",
        "    truth += nothing_pad\n",
        "  elif(diff < 0):\n",
        "    pred += nothing_pad\n",
        "  return pred, truth\n",
        "  \n",
        "use_gpu = True\n",
        "\n",
        "if use_gpu:\n",
        "  retinanet_raw = retinanet_raw.cuda()\n",
        "  retinanet_finetuned = retinanet_finetuned.cuda()\n",
        "\n",
        "retinanet_raw.eval()\n",
        "retinanet_finetuned.eval()\n",
        "\n",
        "unnormalize = UnNormalizer()\n",
        "\n",
        "# fig, ax = plt.subplots(figsize=(20, 10))\n",
        "classcnt = [3 for x in range(8)]\n",
        "sum = 24\n",
        "\n",
        "\n",
        "raw_pred_list = []\n",
        "finetune_pred_list = []\n",
        "gt_raw_list = []\n",
        "gt_finetune_list = []\n",
        "cnt = 1 \n",
        "for idx, data in enumerate(dataloader_val):\n",
        "  # print(int(data['annot'][0][0][4]))\n",
        "\n",
        "  # 3 images for each class\n",
        "  # category = int(data['annot'][0][0][4])\n",
        "  # if(classcnt[category] > 0):\n",
        "  #   classcnt[category] -= 1\n",
        "  #   sum -= 1\n",
        "  # elif sum <= 0:\n",
        "  #   break\n",
        "  # else:\n",
        "  #   continue\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    \n",
        "    img = np.array(255 * unnormalize(data['img'][0, :, :, :])).copy()\n",
        "\n",
        "    img[img<0] = 0\n",
        "    img[img>255] = 255\n",
        "\n",
        "    img = np.transpose(img, (1, 2, 0))\n",
        "\n",
        "    img = cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # st = time.time()\n",
        "    raw_pred = get_bbox(retinanet_raw,img,(0,0,255),data,str(cnt)+\" Model:\")\n",
        "    finetune_pred = get_bbox(retinanet_finetuned,img,(0,255,0),data,str(cnt)+\" Finetuned model:\")\n",
        "    gt_raw = []\n",
        "    gt_finetune = []\n",
        "    # Ground truth\n",
        "    # print(data['annot'])\n",
        "\n",
        "    for bbox in data['annot'][0]:\n",
        "      x1 = int(bbox[0])\n",
        "      y1 = int(bbox[1])\n",
        "      x2 = int(bbox[2])\n",
        "      y2 = int(bbox[3])\n",
        "      label_name = dataset_val.labels[int(bbox[4])]\n",
        "      gt_raw.append(label_name)\n",
        "      gt_finetune.append(label_name)\n",
        "      draw_caption(img, (x1, y1, x2, y2), label_name)\n",
        "      # print(str(cnt)+\" Ground truth:\",label_name)\n",
        "      cv2.rectangle(img, (x1, y1), (x2, y2), color=(255,0,0), thickness=2)\n",
        "      # print(label_name)\n",
        "\n",
        "    raw_pred,gt_raw = equalize(raw_pred,gt_raw)\n",
        "    finetune_pred,gt_finetune = equalize(finetune_pred,gt_finetune)\n",
        "\n",
        "    # print(\"Raw pretrained\",raw_pred,\"Ground\",gt_raw)\n",
        "    # print(\"Finetune pred\",finetune_pred,\"Ground\",gt_finetune)\n",
        "\n",
        "    raw_pred_list += raw_pred\n",
        "    finetune_pred_list += finetune_pred\n",
        "\n",
        "    gt_raw_list += gt_raw\n",
        "    gt_finetune_list += gt_finetune\n",
        "\n",
        "    # print(\"Ground truth\",gt)\n",
        "    # print(\"------------------------\")\n",
        "    # print('Elapsed time: {}'.format(time.time()-st))\n",
        "\n",
        "    # plt.figure(figsize=(20,10)) \n",
        "    # plt.imshow(img)\n",
        "    # plt.savefig(\"images1/\"+str(cnt)+\"classification.png\")\n",
        "    # print(\"---------------------------------\")\n",
        "    cnt += 1\n",
        "    # plt.show()\n",
        "\n",
        "# print(len(raw_pred_list),len(gt_raw_list))\n",
        "# print(len(finetune_pred_list),len(gt_finetune_list))\n",
        "\n",
        "# print((raw_pred_list),(gt_raw_list))\n",
        "# print((finetune_pred_list),(gt_finetune_list))\n",
        "\n",
        "print(confusion_matrix(gt_raw_list,raw_pred_list,labels=[\"bird\",\"bobcat\",\"car\",\"cat\",\"raccoon\",\"rabbit\",\"coyote\",\"squirrel\",\"nothing\"]))\n",
        "print(confusion_matrix(gt_finetune_list,finetune_pred_list,labels=[\"bird\",\"bobcat\",\"car\",\"cat\",\"raccoon\",\"rabbit\",\"coyote\",\"squirrel\",\"nothing\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCDybbM0ZfLo",
        "colab_type": "code",
        "outputId": "42cb60e0-30ef-4590-f32a-442d4383a33b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "\n",
        "\n",
        "parser = {\"csv_classes\":CLASS_LIST, \"csv_val\":VAL_CSV, \"csv_train\":TRAIN_CSV, \"epochs\":15 }\n",
        "\n",
        "dataset_train = CSVDataset(train_file=parser[\"csv_train\"], class_list=parser[\"csv_classes\"],\n",
        "                                   transform=transforms.Compose([Normalizer(), Augmenter(), Resizer()]))\n",
        "dataset_val = CSVDataset(train_file=parser[\"csv_val\"], class_list=parser[\"csv_classes\"],\n",
        "                                     transform=transforms.Compose([Normalizer(), Resizer()]))\n",
        "\n",
        "sampler = AspectRatioBasedSampler(dataset_train, batch_size=2, drop_last=False)\n",
        "dataloader_train = DataLoader(dataset_train, num_workers=3, collate_fn=collater, batch_sampler=sampler)\n",
        "\n",
        "sampler_val = AspectRatioBasedSampler(dataset_val, batch_size=1, drop_last=False)\n",
        "dataloader_val = DataLoader(dataset_val, num_workers=3, collate_fn=collater, batch_sampler=sampler_val)\n",
        "\n",
        "retinanet = model.resnet50(num_classes=dataset_train.num_classes(),)\n",
        "\n",
        "for param in retinanet.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in retinanet.classificationModel.output.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in retinanet.regressionModel.output.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "use_gpu = True\n",
        "\n",
        "if use_gpu:\n",
        "    retinanet = retinanet.cuda()\n",
        "\n",
        "PATH_TO_DATASET = \"/content/drive/My Drive/Deep_Learning_Assignments/Assignment1/Q3/assignment-data\"\n",
        "PATH_TO_WEIGHTS = PATH_TO_DATASET + \"/pretrained_weights_cleaned.pt\"\n",
        "\n",
        "retinanet= torch.load(BEST_WEIGHTS)\n",
        "\n",
        "retinanet = torch.nn.DataParallel(retinanet).cuda()\n",
        "\n",
        "retinanet.training = True\n",
        "\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, retinanet.parameters()), lr=1e-5)\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)\n",
        "\n",
        "loss_hist = collections.deque(maxlen=500)\n",
        "\n",
        "loss_all_epochs = []\n",
        "\n",
        "retinanet.train()\n",
        "retinanet.module.freeze_bn()\n",
        "file1 = open(\"logfreezenew.txt\",\"w\")\n",
        "print('Num training images: {}'.format(len(dataset_train)))\n",
        "\n",
        "mAP = csv_eval.evaluate(dataset_val, retinanet)\n",
        "\n",
        "\n",
        "# best_loss = 100\n",
        "# for epoch_num in range(parser[\"epochs\"]):\n",
        "#     print(\"\\nEpoch\",epoch_num)\n",
        "#     retinanet.train()\n",
        "#     retinanet.module.freeze_bn()\n",
        "\n",
        "#     if epoch_num > 10:\n",
        "#       for param in retinanet.parameters():\n",
        "#         param.requires_grad = True\n",
        "#       optimizer = optim.Adam(filter(lambda p: p.requires_grad, retinanet.parameters()), lr=1e-5)\n",
        "    \n",
        "#     epoch_loss = []\n",
        "\n",
        "#     for iter_num, data in enumerate(dataloader_train):\n",
        "#         try:\n",
        "#             optimizer.zero_grad()\n",
        "\n",
        "#             classification_loss, regression_loss = retinanet([data['img'].cuda().float(), data['annot']])\n",
        "\n",
        "#             classification_loss = classification_loss.mean()\n",
        "#             regression_loss = regression_loss.mean()\n",
        "\n",
        "#             loss = classification_loss + regression_loss\n",
        "\n",
        "#             if bool(loss == 0):\n",
        "#                 continue\n",
        "\n",
        "#             loss.backward()\n",
        "\n",
        "#             torch.nn.utils.clip_grad_norm_(retinanet.parameters(), 0.1)\n",
        "\n",
        "#             optimizer.step()\n",
        "\n",
        "#             loss_hist.append(float(loss))\n",
        "\n",
        "#             epoch_loss.append(float(loss))\n",
        "\n",
        "#             # print(\n",
        "#             #     'Epoch: {} | Iteration: {} | Classification loss: {:1.5f} | Regression loss: {:1.5f} | Running loss: {:1.5f}'.format(\n",
        "#             #         epoch_num, iter_num, float(classification_loss), float(regression_loss), np.mean(loss_hist)))\n",
        "            \n",
        "#             file1.write('Epoch: {} | Iteration: {} | Classification loss: {:1.5f} | Regression loss: {:1.5f} | Running loss: {:1.5f} \\n'.format(\n",
        "#                     epoch_num, iter_num, float(classification_loss), float(regression_loss), np.mean(loss_hist)))\n",
        "            \n",
        "#             del classification_loss\n",
        "#             del regression_loss\n",
        "#         except Exception as e:\n",
        "#             print(e)\n",
        "#             continue\n",
        "\n",
        "#     print('Evaluating dataset')\n",
        "\n",
        "#     mAP = csv_eval.evaluate(dataset_val, retinanet)\n",
        "\n",
        "#     mean_loss = np.mean(epoch_loss)\n",
        "\n",
        "#     scheduler.step(mean_loss)\n",
        "\n",
        "#     loss_all_epochs.append(mean_loss)\n",
        "\n",
        "#     if(mean_loss < best_loss):\n",
        "#         torch.save(retinanet.module, 'freeze_model/{}_retinanet_{}.pt'.format(\"csv\", \"best_model\"))\n",
        "#         best_loss = mean_loss\n",
        "    \n",
        "#     torch.save(retinanet.module, 'freeze_model/{}_retinanet_{}.pt'.format(\"csv\", epoch_num))\n",
        "\n",
        "# retinanet.eval()\n",
        "# file1.close()\n",
        "# torch.save(retinanet, 'freeze_model/model_final.pt')\n",
        "\n",
        "# print(\"Loss curve\")\n",
        "# x_axis = [x for x in range(15)]\n",
        "# plt.plot(x_axis,loss_all_epochs)\n",
        "# plt.xlabel(\"Epochs\")\n",
        "# plt.ylabel(\"Running Loss\")\n",
        "# plt.savefig(\"/content/drive/My Drive/Deep_Learning_Assignments/Assignment1/Q3/dlassignment1/q3/freeze_model/losscurve.png\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num training images: 1153\n",
            "\n",
            "mAP:\n",
            "bird: 0.3391735656587355\n",
            "bobcat: 0.7082753665581452\n",
            "car: 0.9995748299319728\n",
            "cat: 0.6007539827457286\n",
            "raccoon: 0.8516176272969179\n",
            "rabbit: 0.5223462303178512\n",
            "coyote: 0.5125965089552045\n",
            "squirrel: 0.5644058690910918\n",
            "0.637342997569456\n",
            "CPU times: user 5min 3s, sys: 1min 5s, total: 6min 9s\n",
            "Wall time: 5min 12s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}